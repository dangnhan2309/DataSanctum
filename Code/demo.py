from bertopic import BERTopic
from fileprocessor import FileProcessor
from extractors import Extractor
from nltk.corpus import stopwords
import re
from keybert import KeyBERT
from transformers import T5Tokenizer, T5ForConditionalGeneration
import spacy 
from transformers import pipeline
import subprocess

def clean_text_remove_stopwords(text):
    stop_words = set(stopwords.words('english') + ['http', 'https', 'amp', 'com'])
    words = re.findall(r'\b\w+\b', text.lower())
    filtered = [word for word in words if word not in stop_words]
    return ' '.join(filtered)

def show_vnaban(filepath, function):
    try:
        print("\U0001F50D Start to extracting ....")
        processor = FileProcessor()
        mime_type, extractor_name = processor.process_file(filepath)
        print("-" * 50)
        print(f"\U0001F4C4 MIME type xÃ¡c Ä‘á»‹nh: {mime_type}")
        print(f"\U0001F6E0ï¸ HÃ m extractor tÆ°Æ¡ng á»©ng: {extractor_name}")

        if not extractor_name:
            raise ValueError("âš  KhÃ´ng cÃ³ extractor phÃ¹ há»£p cho file nÃ y.")

        extractor = Extractor()
        extractor_func = getattr(extractor, extractor_name, None)
        if not extractor_func:
            raise AttributeError(f"âŒ KhÃ´ng tÃ¬m tháº¥y hÃ m '{extractor_name}' trong Extractor.")

        result = extractor_func(filepath)

        if function.lower() == "raw" or function.lower() == "show":
            print("\U0001F4E5 Káº¿t quáº£ trÃ­ch xuáº¥t ná»™i dung:")
            print(result[:500])
            return result

        elif function.lower() == "test":
            return result

        elif function.lower() == "cleaned":
            print("\U0001F9F9 Start Cleaning ...")
            after_clean = clean_text_remove_stopwords(result)
            print("âœ… Finished Cleaning!")
            print("\U0001F9FE Káº¿t quáº£ sau khi lÃ m sáº¡ch:")
            print(after_clean)
            return after_clean

        else:
            raise ValueError(f"âš  KhÃ´ng há»— trá»£ cháº¿ Ä‘á»™ function: '{function}'")

    except Exception as e:
        print(f"âŒ ÄÃ£ xáº£y ra lá»—i: {e}")
        raise e

    
def load_spacy_model(model_name="en_core_web_sm"):
    try:
        return spacy.load(model_name)
    except OSError:
        print(f"âš ï¸ spaCy model '{model_name}' chÆ°a cÃ³, Ä‘ang táº£i xuá»‘ng...")
        subprocess.run(["python", "-m", "spacy", "download", model_name])
        return spacy.load(model_name)
#khong dung
def extract_first_50_lines(result):
    lines = result.splitlines()  # Chia ná»™i dung thÃ nh cÃ¡c dÃ²ng
    text_50 = "\n".join(lines[:50])  # Láº¥y 50 dÃ²ng Ä‘áº§u vÃ  ná»‘i láº¡i thÃ nh vÄƒn báº£n
    return text_50

def interactive_menu(filepath, model_path):
    while True:
        print("\nğŸ§ª MENU TEST SHOW_VNABAN ğŸ§ª")
        print("1. Hiá»ƒn thá»‹ Raw text (50 dÃ²ng Ä‘áº§u)")
        print("2. Hiá»ƒn thá»‹ text sau khi clean stopwords")
        print("3. Hiá»ƒn thá»‹ cáº£ Raw vÃ  Cleaned")
        print("4. Testing lá»—i model")
        print("0. ThoÃ¡t")

        choice = input("\nğŸ”¢ Chá»n chá»©c nÄƒng (0-5): ").strip()

        if choice == "1":
            print("\nğŸ“œ Hiá»ƒn thá»‹ vÄƒn báº£n gá»‘c:")
            show_vnaban(filepath, "raw")

        elif choice == "2":
            print("\nğŸ§¼ Hiá»ƒn thá»‹ vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch:")
            show_vnaban(filepath, "cleaned")

        elif choice == "3":
            print("\nğŸ“œ VÄƒn báº£n gá»‘c:")
            raw_text = show_vnaban(filepath, "raw")
            print("\nğŸ§¼ VÄƒn báº£n sau khi clean:")
            cleaned = clean_text_remove_stopwords(raw_text)
            print(cleaned)

        elif choice == "4":
            text = show_vnaban(filepath, "raw")
            process_file_for_topic(filepath, model_path, text)

        elif choice == "0":
            print("ğŸ‘‹ Káº¿t thÃºc.")
            break

        else:
            print("âš  Vui lÃ²ng chá»n má»™t sá»‘ tá»« 0 Ä‘áº¿n 5.")

def model_test(model_path):
    print("50"*50)


def load_model(model_path):
    try:
        model = BERTopic.load(model_path)
        print(f"âœ… ÄÃ£ load BERTopic model tá»«: {model_path}")
        return model
    except Exception as e:
        import traceback
        print("âŒ Lá»—i khi load model:")
        traceback.print_exc()  # In toÃ n bá»™ stack trace
        return None
def process_file_for_topic(file_path, model_path,text):
    topic_model = load_model(model_path)
    if not topic_model:
        return
    result = predict_topic(text, topic_model)
    if not result:
        return
    topic_id, topic_words, confidence = result
    print(topic_words)

    topic_namet1,topic_namet2,topic_namet3,topic_namespacy,topic_name_tranformer,topic_namekeybert = generate_topic_name(topic_words)

    display_topic_info(topic_id, topic_words, confidence,topic_namet1,topic_namet2,topic_namet3,topic_namespacy,topic_name_tranformer,topic_namekeybert )
    # 5. Dá»± Ä‘oÃ¡n chá»§ Ä‘á»
def predict_topic(text, topic_model):
    topics, probs = topic_model.transform([text])
    print(f"\nğŸ“˜ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: {topics[0]}")
    print(f"ğŸ“ˆ XÃ¡c suáº¥t: {probs[0]}")

    if topics[0] == -1:
        print("âš  KhÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c chá»§ Ä‘á».")
        return None

    topic_id = topics[0]
    topic_words = topic_model.get_topic(topic_id)
    topic_words_list = [word for word, _ in topic_words[:15]]

    confidence = probs[0].max()
    return topic_id, topic_words_list, confidence


# 6. Táº¡o tÃªn chá»§ Ä‘á»
def generate_topic_name(topic_words_list):
    keywords= topic_words_list
    var1 = generate_topic_t5(keywords, model_name='t5-large')
    var2= generate_topic_t5_small(keywords)
    var3= generate_topic_t5_medium(keywords)
    var4= generate_topic_spacy(keywords)
    var5= generate_topic_transformer_summarizer(keywords)
    var6= generate_topic_keybert(keywords)
    print("\nğŸ§  Gá»£i Ã½ tÃªn chá»§ Ä‘á» tá»« cÃ¡c phÆ°Æ¡ng phÃ¡p:")
    print(f"T5-large: {var1}")
    print(f"T5-small: {var2}")
    print(f"T5-medium: {var3}")
    print(f"SpaCy: {var4}")
    print(f"Transformer (BART): {var5}")
    print(f"KeyBERT: {var6}")
    return var1,var2,var3,var4,var5,var6

def generate_topic_t5(keywords, model_name="t5-large"):
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    input_text = "summarize: " + ", ".join(keywords)
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(input_ids, max_length=10, num_beams=4, early_stopping=True)

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)
def generate_topic_t5_medium(keywords):
    return generate_topic_t5(keywords, model_name="t5-base")  

def generate_topic_t5_small(keywords):
    return generate_topic_t5(keywords, model_name="t5-small")

def generate_topic_keybert(keywords):
    kw_model = KeyBERT()
    joined_text = " ".join(keywords)
    topic = kw_model.extract_keywords(joined_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=1)
    return topic[0][0] if topic else "Unknown Topic"  

def generate_topic_spacy(keywords):
    nlp = load_spacy_model("en_core_web_sm")
    doc = nlp(" ".join(keywords))
    chunks = [chunk.text for chunk in doc.noun_chunks]
    return chunks[0] if chunks else "General Topic" 

def generate_topic_transformer_summarizer(keywords):
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")  # Or "google/pegasus-xsum"
    input_text = " ".join(keywords)
    result = summarizer(input_text, max_length=10, min_length=3, do_sample=False)
    return result[0]['summary_text']

# 7. Hiá»ƒn thá»‹ thÃ´ng tin chá»§ Ä‘á»
def display_topic_info(topic_id, topic_words, confidence,var1,var2,var3,var4,var5,var6):
    print("\nğŸ¯ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n")
    print("-" * 50)
    print(f"ğŸ†” ID: {topic_id}")
    print(f"ğŸ” TÃªn: {var1}")
    print(f"ğŸ” TÃªn2: {var2}")
    print(f"ğŸ” TÃªn3: {var3}")
    print(f"ğŸ” TÃªn4: {var4}")
    print(f"ğŸ” TÃªn5: {var5}")
    print(f"ğŸ” TÃªn6: {var6}")
    print(f"ğŸ’¡ Tá»« khÃ³a: {', '.join(topic_words)}")
    print(f"ğŸ“ˆ Äá»™ tin cáº­y: {confidence:.4f}")
    print("-" * 50)
def main():
    filepath = r"D:\test.txt"
    model_path = r"D:\model\bertopic_model(ver1-50k_Reference)"
    interactive_menu(filepath,model_path)
    # topic_model = BERTopic.load(model_path)
    # topics = topic_model.get_topics()
    # print(f"Sá»‘ lÆ°á»£ng chá»§ Ä‘á» Ä‘Æ°á»£c tÃ¬m ra: {len(topics)}")

if __name__ == "__main__":
    main()
