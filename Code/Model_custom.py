import os
import json
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

# T·∫£i t√†i nguy√™n NLTK
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# ==== ƒê∆∞·ªùng d·∫´n & c·∫•u h√¨nh ====
json_file = r"D:\DataSanctum\model\Dataset\abstracts_80k_from_2020.json"
save_dir = r"D:/model"
model_path = os.path.join(save_dir, "bertopic_model(ver1-50k_Reference)")

if not os.path.exists(save_dir):
    os.makedirs(save_dir)
    print(f"‚úÖ T·∫°o th∆∞ m·ª•c: {save_dir}")

# ==== ƒê·ªçc d·ªØ li·ªáu ====
with open(json_file, "r", encoding="utf-8") as file:
    abstracts = json.load(file)

# L·∫•y 10.000 m·∫´u (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)
abstracts = abstracts[:50000]
print(f"üìå S·ªë l∆∞·ª£ng abstracts thu th·∫≠p ƒë∆∞·ª£c: {len(abstracts)}")

# ==== Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ====
stop_words = set(stopwords.words('english')) | {'http', 'https', 'amp', 'com'}
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w.isalnum() and w not in stop_words]
    return ' '.join(tokens)

abstracts = [preprocess_text(a) for a in abstracts]

# ==== C·∫•u h√¨nh m√¥ h√¨nh ====
embedding_model = SentenceTransformer('all-mpnet-base-v2')

umap_model = UMAP(n_neighbors=30, min_dist=0.1, metric='cosine')  # tƒÉng n_neighbors
hdbscan_model = HDBSCAN(min_cluster_size=50, min_samples=15, prediction_data=True)


vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words='english')  # gi·∫£m n-gram

# ==== Hu·∫•n luy·ªán BERTopic ====
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    language='english',
    verbose=True
)

topics, probs = topic_model.fit_transform(abstracts)

# ==== L∆∞u m√¥ h√¨nh ====
try:
    topic_model.save(model_path, save_embedding_model=True)
    print(f"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {model_path}")
except Exception as e:
    print(f"‚ùå L·ªói khi l∆∞u m√¥ h√¨nh: {e}")

# ==== Ki·ªÉm tra load l·∫°i ====
try:
    loaded_model = BERTopic.load(model_path)
    print(f"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c load l·∫°i t·ª´: {model_path}")
except Exception as e:
    print(f"‚ùå L·ªói khi load m√¥ h√¨nh: {e}")

# ==== Ki·ªÉm tra k·∫øt qu·∫£ ====
print(topic_model.get_topic_info().head())
