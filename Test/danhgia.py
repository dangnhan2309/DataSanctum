from bertopic import BERTopic
from keybert import KeyBERT

print("ğŸ“Œ Báº¯t Ä‘áº§u cháº¡y chÆ°Æ¡ng trÃ¬nh...")

# ÄÆ°á»ng dáº«n Ä‘áº¿n mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u
model_path = r"D:\model\bertopic_model(ver3-50k_Reference)"

# Táº£i láº¡i mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u
try:
    topic_model = BERTopic.load(model_path)
    print(f"âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c load thÃ nh cÃ´ng tá»«: {model_path}")
except Exception as e:
    print(f"âŒ Lá»—i khi load mÃ´ hÃ¬nh: {e}")
    exit()

# VÄƒn báº£n máº«u Ä‘á»ƒ kiá»ƒm tra
sample_text = """
Decision trees are one of the most interpretable models in machine learning,
providing a clear visual representation of the decision-making process. These
models are structured as binary trees, where internal nodes represent tests on
features, branches represent outcomes of these tests, and leaf nodes represent
class labels or predicted values. Decision trees can be used for both
classification and regression tasks. One of their main advantages is their
ability to handle both numerical and categorical data without requiring
extensive preprocessing. However, they are prone to overfitting, especially when
grown to deep levels. To counteract this, techniques like pruning and ensemble
methods such as random forests are commonly applied. This paper presents the
construction, optimization, and limitations of decision trees, followed by  
comparisons with other models on standard datasets.
"""

# Dá»± Ä‘oÃ¡n chá»§ Ä‘á»
predicted_topic, predicted_prob = topic_model.transform([sample_text])

# Kiá»ƒm tra káº¿t quáº£ dá»± Ä‘oÃ¡n
if predicted_topic[0] != -1:
    topic_id = predicted_topic[0]  # ID chá»§ Ä‘á» dá»± Ä‘oÃ¡n
    topic_words = topic_model.get_topic(topic_id)  # Láº¥y danh sÃ¡ch tá»« quan trá»ng trong chá»§ Ä‘á»
    topic_words_list = [word for word, _ in topic_words[:20]]  # Láº¥y 20 tá»« Ä‘áº¡i diá»‡n

    print(f"\nğŸ”¹ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: {topic_id}")
    print(f"ğŸ“Œ Äá»™ tin cáº­y: {predicted_prob[0].max():.4f}")
    print(f"ğŸ”¹ Tá»« khÃ³a chá»§ Ä‘á»: {', '.join(topic_words_list)}")

else:
    print("âŒ KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh chá»§ Ä‘á» phÃ¹ há»£p cho vÄƒn báº£n nÃ y.")
    exit()

# ------------------------------------------------------------------------------------
# # PhÃ¢n tÃ­ch tÃªn chá»§ Ä‘á» báº±ng KeyBERT + spaCy
# nlp = spacy.load("en_core_web_sm")
# kw_model = KeyBERT(model='all-MiniLM-L6-v2')

# def generate_topic_keybert_spacy(topic_words):
#     text = ' '.join(topic_words)
#     keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=2)
#     return ", ".join([kw[0].title() for kw in keywords])

# # Táº¡o tÃªn chá»§ Ä‘á» tá»« danh sÃ¡ch tá»«
# kb_name = generate_topic_keybert_spacy(topic_words_list)

# # In káº¿t quáº£ cuá»‘i cÃ¹ng
# print("-" * 100)
# print("{:<60} | {:<30}".format("TOPIC WORDS", "KEYBERT+spaCy"))
# print("-" * 100)
# print("{:<60} | {:<30}".format(", ".join(topic_words_list), kb_name))
# print("-" * 100)